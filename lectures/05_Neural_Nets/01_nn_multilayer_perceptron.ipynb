{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Nets and Multi-layer Perceptrons\n",
    "\n",
    "In this notebook we will learn how we could use a neural network to predict cancer based on medical images. \n",
    "\n",
    "#### Start by importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Today we will work with a dataset on breast cancer, also built into the scikit-learn datasets.\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# print('Dataset raw object', cancer)\n",
    "print('Dataset description', cancer['DESCR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into our training and testing XY sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# However, the MLP method doesn't automatically scale the data, so let's do that.\n",
    "# Here I show how to do it manually with Numpy functions, though there are alternative\n",
    "# built-in methods within scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using numpy functions, compute the mean value per feature on the training set and the STD.\n",
    "# May want to remind ourselves what the X_train looks like.\n",
    "print('X_train', X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The power of Numpy starts to be evident here.\n",
    "# We can calculate the mean of each column (we specify we want sums each column, which means\n",
    "# summing down the first axis). The .mean() function from Numpy as insanely efficient\n",
    "# for processing very big data and is ready for, e.g., super computers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_on_train = X_train.mean(axis=0)\n",
    "\n",
    "print('mean_on_train', mean_on_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the .std() function is similarily powerful/fast.\n",
    "std_on_train = X_train.std(axis=0)\n",
    "\n",
    "print('std_on_train', std_on_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Still using the Numpy awesomeness,\n",
    "# subtract the mean, and scale by inverse standard deviation,\n",
    "# making it  mean=0 and std=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_scaled = (X_train - mean_on_train) / std_on_train\n",
    "X_test_scaled = (X_test - mean_on_train) / std_on_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using this new scaled training data, we are ready to define a Neural Net,\n",
    "# Known here as a Multi-Layer-Perceptron (MLP) classifier.\n",
    "# Because this next line hides away millions of other lines of code, you may want\n",
    "# to explore it. In Pycharm, you can navigate to a function's definition by placing\n",
    "# your cursor in the function and press f-12. Try it! The  best documentation\n",
    "# is often the code itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=0)\n",
    "\n",
    "# Now fit it with the scaled X and y TRAINING data.\n",
    "mlp.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assess its accuracy on the TRAINING and the TESTING data.\n",
    "# Notice here also I'm introducing another convenient way of combining strings\n",
    "# and numbers. The {:.2f} specifies a placeholder for a 2-digit representation\n",
    "# of a floating point number. The Format method then places that floating point value\n",
    "# into that placeholder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = mlp.score(X_train_scaled, y_train)\n",
    "score_test = mlp.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(score_train))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Other concepts discussed earlier, such as regularization and Cross-Validation, also apply here.\n",
    "# To illustrate, here we will set the alpha parameter to include a regulariazation term.\n",
    "# The cross-validation method is often defined by the model itself and will be used automatically\n",
    "# when you call .fit().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# But what does a MLP Neural Net actually LOOK like?\n",
    "# Plot the coeffs_ array to find out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\n",
    "plt.yticks(range(30), cancer.feature_names)\n",
    "plt.xlabel(\"Columns in weight matrix\")\n",
    "plt.ylabel(\"Input feature\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('8222env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db313e0ad7b6749a6d098fb61fddaded88cbd823278030b75fa0893942c8f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
