{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Automatic feature selection with LASSO regression\n",
    "\n",
    "In this notebook we will learn how LASSO (Least Absolute Shrinkage and Selection Operator) regression works and how it can assist in automatically selecting which variables should be included using a **Cross-Validation** perspective.\n",
    "\n",
    "#### Start by importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import statsmodels\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load dataset and inspect it\n",
    "\n",
    "Again we're going to use our diabetes dataset. Inspect it again just to remind yourself\n",
    "what is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "print(diabetes['DESCR'])\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Select subset of data\n",
    "\n",
    "To speed up calculation, we're going to just use the first 150 observations\n",
    "using numpy slice notation to grab them out of the X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = X[:150]\n",
    "y = y[:150]\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run OLS first (for comparison)\n",
    "\n",
    "Remember the standard Sklearn model steps:\n",
    "\n",
    "1. create the model object\n",
    "2. call the object's fit method.\n",
    "3. use the fitted model to predict something.\n",
    "4. assess the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "model_ols = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model_ols.fit(X, y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_hat = model_ols.predict(X)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", model_ols.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error:\", mean_squared_error(y, y_hat))\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination:\", r2_score(y, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do it again in the econometrics style\n",
    "\n",
    "Recall that the package statsmodels is closer to the econometrician's way of doing things. We're going to quickly repeat the steps above but with Statsmodels so we can view it in a nice table form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_with_constant = statsmodels.api.add_constant(X)\n",
    "result = OLS(y, x_with_constant).fit().summary()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot y and y_hat\n",
    "\n",
    "Let's also plot y and y_hat compared to one of the most important variables, BMI. We'll see both y and y_hat resemble each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs (comparing 1 variable (BMI in column 3) to y and y_hat\n",
    "plt.scatter(X[:, 3], y, color=\"black\")\n",
    "plt.scatter(X[:, 3], y_hat, color=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Switch to LASSO\n",
    "\n",
    "Now that we've spent all this time setting up our python environment and getting sklearn, it's almost a trivial step in many cases to try out the latest-and-greatest model.\n",
    "\n",
    "#### Create a LASSO model object\n",
    "\n",
    "Today's goal, however, is to do Lasso on this same dataset.\n",
    "To start, lets create a Lasso object. Notice that we are not\n",
    "setting the alpha/gamma value when we create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha=1.0, random_state=0, max_iter=10000) # Note, alpha is set by default to 1.0 so we could have omitted it here (though I kept it in to make it clear)\n",
    "print(model_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the LASSO\n",
    "\n",
    "Call the lasso.fit() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso.fit(X, y)\n",
    "print(model_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lasso = model_lasso.predict(X)\n",
    "print('y_hat_lasso', y_hat_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot it too to compare it with the OLS plot from above\n",
    "\n",
    "What do you see. Is this expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X[:, 3], y, color=\"black\")\n",
    "plt.scatter(X[:, 3], y_hat_lasso, color=\"blue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the actual coefficients created\n",
    "\n",
    "Class question: How are they different? And how are they similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_lasso.coef_)\n",
    "print(model_ols.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01\n",
    "\n",
    "Use a loop to identify the best value of alpha, as measured by r-squared. Discussion question for once you're done: what was the optimal alpha and why does this make sense? How does this compare to OLS? Why is it that way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise work space\n",
    "\n",
    "# Starter code: keyt parts omitted.\n",
    "alphas = np.logspace(-5, -0.05, 30)\n",
    "for SOMETHING in SOMETHING_ELSE:\n",
    "    model_lasso = Lasso(alpha=alpha, random_state=0, max_iter=10000)\n",
    "    # LINE OMIITTED\n",
    "    # LINE OMIITTED\n",
    "    r2 = r2_score(y, y_hat_lasso)\n",
    "    print('R2 for alpha ' + str(alpha) + ': ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Operationalizing CV with GridSearch\n",
    "\n",
    "It seems a little weird to be automatically finding the best model. If we were just applying this to the dataset a single time, this would indeed be p-hacking to the extreme. However, showing its performance on UNSEEN data is quite the opposite of p-hacking.\n",
    "\n",
    "Here, we're going to operationalize our method for finding th ebest model by using GridSearch. We are going to test a variety of different alphas, similar to above. Define them here using numpy logspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, -0.5, 30)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are going to be passing this range of tuning parameters to a GridSearch function\n",
    "that will test which works best when cross-validation methods are applied.\n",
    "First though, we have to put the alphas into the form the GridSearchCV funciton\n",
    "Expects, which is a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuning_parameters = [{'alpha': alphas}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall that CV works by calculating the fit quality of different folds of the training data. Here we will just use 5 folds. GridSearchCV will automatically implement the folding and testing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create the lasso_cv object from the lasso object\n",
    "\n",
    "Finally, we have all our objects ready to pass to the GridSearchVC function which will Give us back a classifier object. Notice that we're reusing that model_lasso objectg we created above. The difference is that we will be systematically  handing different parameters from the tuning_parameters list into the model_lasso object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_lasso_cv = GridSearchCV(model_lasso, tuning_parameters, cv=n_folds, refit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Fit the lasso_cv object\n",
    "\n",
    "When we call the model_lasso_cv.fit() method, we will iteratively be calling the Lasso.fit() with different permutations of\n",
    "tuned parameters and then will return the classifier with the best CV fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_lasso_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The classifier object now has a variety of diagnostic metrics, reporting back on different folds within the Cross Validation. Take a look at them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('model_lasso_cv keys returned:', model_lasso_cv.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some relevant results are as below, which we'll extract and assign to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = model_lasso_cv.cv_results_['mean_test_score']\n",
    "scores_std = model_lasso_cv.cv_results_['std_test_score']\n",
    "\n",
    "print('scores', scores)\n",
    "print('scores_std', scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 02: \n",
    "\n",
    "With your table, explore the scores and alphas lists we've created. Identify which alpha is the best, based on the MSE score returned. \n",
    "\n",
    "One way to consider doing this would be to create a for loop to iterate through a range(len(scores)): object, reporting the alphas and scores. save the optimal alpha as a new variable called chosen_alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Exercise work space\n",
    "\n",
    "output_dict = {}\n",
    "for i in OMITTED_CODE:\n",
    "    output_dict[alphas[i]] = scores[i]\n",
    "    \n",
    "chosen_alpha = max(output_dict, key=output_dict.get)\n",
    "\n",
    "print('best_alpha', chosen_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Use the built-in attributes to get the best alpha\n",
    "\n",
    "Fortunately, the authors provide a useful  best_params_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('best_parameters:', model_lasso_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Extract the best alpha, which we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chosen_alpha = model_lasso_cv.best_params_['alpha']\n",
    "print('chosen_alpha', chosen_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Rerun LASSO with the best alpha\n",
    "\n",
    "Now we can rerun a vanilla (no CV) version of Lasso with that specific alpha.\n",
    "This will return, for instance, a .coef_ list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_lasso_cv_2 = Lasso(alpha=chosen_alpha, random_state=0, max_iter=10000).fit(X, y)\n",
    "\n",
    "print(\"coefficients\", model_lasso_cv_2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Simply looking at the coefficients tells us which are to be included.\n",
    "Question: How will we know just by looking?\n",
    "\n",
    "#### Extract the feature names and colum indices of the features that Lasso has selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selected_coefficient_labels = []\n",
    "selected_coefficient_indices = []\n",
    "for i in range(len(model_lasso_cv_2.coef_)):\n",
    "    print('Coefficient', feature_names[i], 'was', model_lasso_cv_2.coef_[i])\n",
    "    if abs(model_lasso_cv_2.coef_[i]) > 0:\n",
    "        selected_coefficient_labels.append(feature_names[i])\n",
    "        selected_coefficient_indices.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This process led us to the following selected_coefficient_labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('selected_coefficient_labels', selected_coefficient_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot the scores versus the alphas\n",
    "\n",
    "For fun, let's plot the alphas, scores and a confidence range.\n",
    "What does this show us about the optimal alpha and how it varies with score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A fun aspect of the k-fold approach is you can get a measure of the std_errors involved. Plot those below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Plot the confidence band and the maximum score\n",
    "\n",
    "alpha=0.2 controls the translucency of the fill color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch back to slides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Post-LASSO\n",
    "\n",
    "Finally, now that we have our selected labels, we can use them to select the numpy array columns that we want to use for a post-LASSO run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_x = X[:, selected_coefficient_indices]\n",
    "new_x = statsmodels.api.add_constant(new_x)\n",
    "print('new_x', new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug this new x matrix into our statsmodels OLS function and print that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = OLS(y, new_x).fit().summary()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class discussion\n",
    "\n",
    "How does the r-squared of this model compare to the one we did at the start of the lecture?\n",
    "\n",
    "Given the above, how is the LASSO approach better than a vanilla OLS?\n",
    "\n",
    "Look at the adjusted R-squared. How does that compare across models. In what ways is the adjusted R-squared similar the CV approach?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('8222env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db313e0ad7b6749a6d098fb61fddaded88cbd823278030b75fa0893942c8f77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
